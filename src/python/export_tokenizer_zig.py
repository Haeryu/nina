import json
import os
from tokenizers import ByteLevelBPETokenizer
import requests

# === 설정 ===
vocab_url = "https://huggingface.co/gpt2/raw/main/vocab.json"
merges_url = "https://huggingface.co/gpt2/raw/main/merges.txt"
vocab_file = "datas/vocab.json"
merges_file = "datas/merges.txt"
corpus_files = ["datas/corpus.txt", "datas/tiny.txt"]
zig_output_dir = "src/tokenizer"
special_tokens = [
    "<|endoftext|>", "<pad>",
    "<user>", "</user>",
    "<bot>", "</bot>",
    "<action>", "</action>",
    "<emotion>", "</emotion>",
    "<thought>", "</thought>",
    "<narration>", "</narration>",
    "<location>", "</location>"
    "<info>", "</info>"
]

gpt2_vocab_size = 50257

# === tokenizer 파일 다운로드 (없을 경우) ===
os.makedirs("datas", exist_ok=True)

if not os.path.exists(vocab_file):
    print("Downloading vocab.json...")
    r = requests.get(vocab_url)
    with open(vocab_file, "wb") as f:
        f.write(r.content)

if not os.path.exists(merges_file):
    print("Downloading merges.txt...")
    r = requests.get(merges_url)
    with open(merges_file, "wb") as f:
        f.write(r.content)

    
def escape_for_zig(s):
    result = []
    for c in s:
        if c == '"':
            result.append('\\"')
        elif c == '\\':
            result.append('\\\\')
        elif 0x20 <= ord(c) <= 0x7E:
            result.append(c)
        else:
            result.append(f"\\u{{{ord(c):X}}}")
    return ''.join(result)


# === 출력 디렉토리 생성 ===
os.makedirs(zig_output_dir, exist_ok=True)

# === GPT-2 tokenizer 로딩 및 파인튜닝 ===
tokenizer = ByteLevelBPETokenizer(vocab_file, merges_file, add_prefix_space=False)
tokenizer.train(files=corpus_files, vocab_size=gpt2_vocab_size, special_tokens=special_tokens)

# === vocab 로딩 ===
with open(vocab_file, "r", encoding="utf-8") as vf:
    vocab = json.load(vf)

sorted_vocab_by_id = sorted(vocab.items(), key=lambda x: x[1])

# === encoder_data.zig ===
with open(f"{zig_output_dir}/encoder_data.zig", "w", encoding="utf-8") as f:
    f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
    f.write('// Generated by export_tokenizer_zig.py\n\n')
    f.write("pub const encoder_data = [_]struct { []const u8, usize }{\n")
    for token, idx in sorted_vocab_by_id:
        token_escaped =  escape_for_zig(token)
        f.write(f'    .{{ "{token_escaped}", {idx} }},\n')
    f.write("};\n")

# === merges_data.zig ===
with open(f"{zig_output_dir}/merges_data.zig", "w", encoding="utf-8") as f:
    f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
    f.write('// Generated by export_tokenizer_zig.py\n\n')
    f.write("pub const merges_data = [_]struct { []const u8, []const u8 }{\n")
    with open(merges_file, "r", encoding="utf-8") as mf:
        for line in mf:
            line = line.strip()
            if line.startswith("#") or not line:
                continue
            a, b = line.split()
            a_escaped =  escape_for_zig(a)
            b_escaped =  escape_for_zig(b)
            f.write(f'    .{{ "{a_escaped}", "{b_escaped}" }},\n')
    f.write("};\n")

# === decoder_map.zig ===
with open(f"{zig_output_dir}/decoder_map.zig", "w", encoding="utf-8") as f:
    f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
    f.write('// Generated by export_tokenizer_zig.py\n\n')
    f.write("pub const decoder_map = [_][]const u8{\n")
    for token, _ in sorted_vocab_by_id:
        token_escaped = escape_for_zig(token) 
        f.write(f'    "{token_escaped}",\n')
    f.write("};\n")

# === encoder_map.zig (StaticStringMap) ===
with open(f"{zig_output_dir}/encoder_map.zig", "w", encoding="utf-8") as f:
    f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
    f.write('// Generated by export_tokenizer_zig.py\n\n')
    f.write('const std = @import("std");\n')
    f.write('const StaticStringMap = std.StaticStringMap;\n')
    f.write('const encoder_data = @import("encoder_data.zig").encoder_data;\n\n')
    f.write('pub const encoder_map = StaticStringMap(usize).initComptime(encoder_data);\n')

