import json
import os
from tokenizers import ByteLevelBPETokenizer
import requests
import unicodedata
import re
import struct

# === 설정 ===
vocab_url = "https://huggingface.co/gpt2/raw/main/vocab.json"
merges_url = "https://huggingface.co/gpt2/raw/main/merges.txt"
vocab_file = "datas/vocab.json"
merges_file = "datas/merges.txt"
raw_corpus_files = [
    # "datas/harry_potter/01 Harry Potter and the Sorcerers Stone.txt", 
    # "datas/harry_potter/02 Harry Potter and the Chamber of Secrets.txt", 
    # "datas/harry_potter/03 Harry Potter and the Prisoner of Azkaban.txt", 
    # "datas/harry_potter/04 Harry Potter and the Goblet of Fire.txt", 
    # "datas/harry_potter/05 Harry Potter and the Order of the Phoenix.txt", 
    # "datas/harry_potter/06 Harry Potter and the Half-Blood Prince.txt", 
    # "datas/harry_potter/07 Harry Potter and the Deathly Hallows.txt",       
    # "datas/corpus.txt"      
    "datas/tiny.txt"
                    ]
zig_output_dir = "src/tokenizer"
model_prefix = "my_bpe"

special_tokens = [
    "<|endoftext|>", "<pad>", "<unk>",
    "<user>", "</user>",
    "<bot>", "</bot>",
    "<action>", "</action>",
    "<emotion>", "</emotion>",
    "<thought>", "</thought>",
    "<narration>", "</narration>",
    "<location>", "</location>",
    "<info>", "</info>"
]

gpt2_vocab_size = 50257

def download_tokenizer_files():
    os.makedirs("datas", exist_ok=True)

    if not os.path.exists(vocab_file):
        print("Downloading vocab.json...")
        r = requests.get(vocab_url)
        with open(vocab_file, "wb") as f:
            f.write(r.content)

    if not os.path.exists(merges_file):
        print("Downloading merges.txt...")
        r = requests.get(merges_url)
        with open(merges_file, "wb") as f:
            f.write(r.content)

def escape_for_zig(s):
    result = []
    for c in s:
        if c == '"':
            result.append('\\"')
        elif c == '\\':
            result.append('\\\\')
        elif 0x20 <= ord(c) <= 0x7E:
            result.append(c)
        else:
            #result.append(f"\\u{{{ord(c):X}}}")
            result.append(c)
    return ''.join(result)


def clean_text(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = text.replace('\ufeff', '')
    text = text.replace("\r\n", "\n").replace("\r", "\n")   
    replacements = {
        "“": '"', "”": '"', "‘": "'", "’": "'",
        "—": "-", "–": "-", "…": "...",
    }
    for k, v in replacements.items():
        text = text.replace(k, v)
    text = re.sub(r"[^\x20-\x7E\n\t]", "", text)
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def clean_and_save_files(files):
    cleaned_paths = []
    for path in files:
        with open(path, "r", encoding="utf-8", newline="\n") as rf:
            text = rf.read()
        cleaned_text = clean_text(text)
        out_path = path.replace(".txt", ".cleaned.txt")
        with open(out_path, "w", encoding="utf-8", newline="\n") as wf:
            wf.write(cleaned_text)
        cleaned_paths.append(out_path)
    return cleaned_paths

def train_tokenizer(corpus_files):
    tokenizer = ByteLevelBPETokenizer(vocab_file, merges_file, add_prefix_space=False)
    tokenizer.train(files=corpus_files, vocab_size=gpt2_vocab_size, special_tokens=special_tokens)
    tokenizer.save_model("datas", model_prefix)
    return ByteLevelBPETokenizer(
        f"datas/{model_prefix}-vocab.json",
        f"datas/{model_prefix}-merges.txt",
        add_prefix_space=False
    )

def export_zig_files(tokenizer):
    os.makedirs(zig_output_dir, exist_ok=True)
    with open(f"datas/{model_prefix}-vocab.json", "r", encoding="utf-8") as vf:
        vocab = json.load(vf)
    sorted_vocab_by_id = sorted(vocab.items(), key=lambda x: x[1])

    with open(f"{zig_output_dir}/encoder_data.zig", "w", encoding="utf-8") as f:
        f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
        f.write('// Generated by export_tokenizer_zig.py\n\n')
        f.write("pub const encoder_data = [_]struct { []const u8, usize }{\n")
        for token, idx in sorted_vocab_by_id:
            f.write(f'    .{{ "{escape_for_zig(token)}", {idx} }},\n')
        f.write("};\n")

    with open(f"{zig_output_dir}/merges_data.zig", "w", encoding="utf-8") as f:
        f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
        f.write('// Generated by export_tokenizer_zig.py\n\n')
        f.write("pub const merges_data = [_]struct { []const u8, []const u8 }{\n")
        with open(f"datas/{model_prefix}-merges.txt", "r", encoding="utf-8") as mf:
            for line in mf:
                line = line.strip()
                if line.startswith("#") or not line:
                    continue
                a, b = line.split()
                f.write(f'    .{{ "{escape_for_zig(a)}", "{escape_for_zig(b)}" }},\n')
        f.write("};\n")

    with open(f"{zig_output_dir}/decoder_map.zig", "w", encoding="utf-8") as f:
        f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
        f.write('// Generated by export_tokenizer_zig.py\n\n')
        f.write("pub const decoder_map = [_][]const u8{\n")
        for token, _ in sorted_vocab_by_id:
            f.write(f'    "{escape_for_zig(token)}",\n')
        f.write("};\n")

    # with open(f"{zig_output_dir}/encoder_map.zig", "w", encoding="utf-8") as f:
    #     f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
    #     f.write('// Generated by export_tokenizer_zig.py\n\n')
    #     f.write('const std = @import("std");\n')
    #     f.write('const StaticStringMap = std.StaticStringMap;\n')
    #     f.write('const encoder_data = @import("encoder_data.zig").encoder_data;\n\n')
    #     f.write('pub const encoder_map: StaticStringMap(usize) =.initComptime(encoder_data);\n')

def save_tokenized_ids(tokenizer, cleaned_files, output_path="datas/bin/token_ids.bin"):
    all_ids = []
    for path in cleaned_files:
        with open(path, "r", encoding="utf-8") as f:
            text = f.read()
        ids = tokenizer.encode(text).ids
        all_ids.extend(ids)

    with open(output_path, "wb") as f:
        f.write(struct.pack("<Q", len(all_ids)))  # total count (usize)
        for id in all_ids:
            f.write(struct.pack("<Q", id))  # each usize token id
    print(f"✅ Saved {len(all_ids)} token ids to {output_path}")


if __name__ == "__main__":
    download_tokenizer_files()
    cleaned_files = clean_and_save_files(raw_corpus_files)
    tokenizer = train_tokenizer(cleaned_files)
    export_zig_files(tokenizer)
    save_tokenized_ids(tokenizer, cleaned_files)
    # print(tokenizer.token_to_id(' '))
