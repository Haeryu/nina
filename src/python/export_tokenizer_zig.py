from tokenizers import Tokenizer, models, trainers, pre_tokenizers
import json
import os

# === 설정 ===
vocab_size = 1000
corpus_files = ["corpus.txt"]
zig_output_dir = "src/tokenizer"
prefix = "my_bpe"

# === 출력 디렉토리 생성
os.makedirs(zig_output_dir, exist_ok=True)

# === 토크나이저 학습 ===
tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=["<unk>"])
tokenizer.train(corpus_files, trainer)

# === 저장용 JSON + TXT 파일 생성
tokenizer.model.save(".", prefix)  # my_bpe-vocab.json, my_bpe-merges.txt

# === vocab 불러오기
with open(f"{prefix}-vocab.json", "r", encoding="utf-8") as vf:
    vocab = json.load(vf)

sorted_vocab_by_id = sorted(vocab.items(), key=lambda x: x[1])   # for encoder/decoder
sorted_vocab_by_token = sorted(vocab.items(), key=lambda x: x[0])  # for StaticStringMap if needed

# === encoder_data.zig
with open(f"{zig_output_dir}/encoder_data.zig", "w", encoding="utf-8") as f:
    f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
    f.write('// Generated by export_tokenizer_zig.py\n\n')
    f.write("pub const encoder_data = [_]struct { []const u8, usize }{\n")
    for token, idx in sorted_vocab_by_id:
        safe_token = token.encode("utf-8").decode("unicode_escape").replace('"', '\\"')
        f.write(f'    .{{ "{safe_token}", {idx} }},\n')
    f.write("};\n")

# === merges_data.zig
with open(f"{zig_output_dir}/merges_data.zig", "w", encoding="utf-8") as f:
    f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
    f.write('// Generated by export_tokenizer_zig.py\n\n')
    f.write("pub const merges_data = [_]struct { []const u8, []const u8 }{\n")
    with open(f"{prefix}-merges.txt", "r", encoding="utf-8") as mf:
        for line in mf:
            line = line.strip()
            if line.startswith("#") or line == "":
                continue
            parts = line.split()
            if len(parts) != 2:
                continue
            a = parts[0].encode("utf-8").decode("unicode_escape").replace('"', '\\"')
            b = parts[1].encode("utf-8").decode("unicode_escape").replace('"', '\\"')
            f.write(f'    .{{ "{a}", "{b}" }},\n')
    f.write("};\n")

# === decoder_data.zig
with open(f"{zig_output_dir}/decoder_map.zig", "w", encoding="utf-8") as f:
    f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
    f.write('// Generated by export_tokenizer_zig.py\n\n')
    f.write("pub const decoder_map = [_][]const u8{\n")
    for token, _ in sorted_vocab_by_id:
        safe_token = token.encode("utf-8").decode("unicode_escape").replace('"', '\\"')
        f.write(f'    "{safe_token}",\n')
    f.write("};\n")

# === encoder_static.zig (StaticStringMap with encoder_data.zig)
with open(f"{zig_output_dir}/encoder_map.zig", "w", encoding="utf-8") as f:
    f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
    f.write('// Generated by export_tokenizer_zig.py\n\n')
    f.write('const std = @import("std");\n')
    f.write('const StaticStringMap = std.StaticStringMap;\n')
    f.write('const encoder_data = @import("encoder_data.zig").encoder_data;\n\n')
    f.write('pub const encoder_map = StaticStringMap(usize).initComptime(encoder_data);\n')