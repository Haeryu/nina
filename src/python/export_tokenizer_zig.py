import json
import os
from tokenizers import ByteLevelBPETokenizer
import requests

# === 설정 ===
vocab_url = "https://huggingface.co/gpt2/raw/main/vocab.json"
merges_url = "https://huggingface.co/gpt2/raw/main/merges.txt"
vocab_file = "datas/vocab.json"
merges_file = "datas/merges.txt"
corpus_files = ["datas/corpus.txt", "datas/tiny.txt"]
zig_output_dir = "src/tokenizer"
model_prefix = "my_bpe"

special_tokens = [
    "<|endoftext|>", "<pad>", "<unk>",
    "<user>", "</user>",
    "<bot>", "</bot>",
    "<action>", "</action>",
    "<emotion>", "</emotion>",
    "<thought>", "</thought>",
    "<narration>", "</narration>",
    "<location>", "</location>",
    "<info>", "</info>"
]

gpt2_vocab_size = 50257

def download_tokenizer_files():
    os.makedirs("datas", exist_ok=True)

    if not os.path.exists(vocab_file):
        print("Downloading vocab.json...")
        r = requests.get(vocab_url)
        with open(vocab_file, "wb") as f:
            f.write(r.content)

    if not os.path.exists(merges_file):
        print("Downloading merges.txt...")
        r = requests.get(merges_url)
        with open(merges_file, "wb") as f:
            f.write(r.content)

def escape_for_zig(s):
    result = []
    for c in s:
        if c == '"':
            result.append('\\"')
        elif c == '\\':
            result.append('\\\\')
        elif 0x20 <= ord(c) <= 0x7E:
            result.append(c)
        else:
            result.append(f"\\u{{{ord(c):X}}}")
    return ''.join(result)

def train_tokenizer():
    tokenizer = ByteLevelBPETokenizer(vocab_file, merges_file, add_prefix_space=False)
    tokenizer.train(files=corpus_files, vocab_size=gpt2_vocab_size, special_tokens=special_tokens)
    tokenizer.save_model("datas", model_prefix)
    return ByteLevelBPETokenizer(
        f"datas/{model_prefix}-vocab.json",
        f"datas/{model_prefix}-merges.txt",
        add_prefix_space=False
    )

def export_zig_files(tokenizer):
    os.makedirs(zig_output_dir, exist_ok=True)
    with open(f"datas/{model_prefix}-vocab.json", "r", encoding="utf-8") as vf:
        vocab = json.load(vf)
    sorted_vocab_by_id = sorted(vocab.items(), key=lambda x: x[1])

    with open(f"{zig_output_dir}/encoder_data.zig", "w", encoding="utf-8") as f:
        f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
        f.write('// Generated by export_tokenizer_zig.py\n\n')
        f.write("pub const encoder_data = [_]struct { []const u8, usize }{\n")
        for token, idx in sorted_vocab_by_id:
            f.write(f'    .{{ "{escape_for_zig(token)}", {idx} }},\n')
        f.write("};\n")

    with open(f"{zig_output_dir}/merges_data.zig", "w", encoding="utf-8") as f:
        f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
        f.write('// Generated by export_tokenizer_zig.py\n\n')
        f.write("pub const merges_data = [_]struct { []const u8, []const u8 }{\n")
        with open(f"datas/{model_prefix}-merges.txt", "r", encoding="utf-8") as mf:
            for line in mf:
                line = line.strip()
                if line.startswith("#") or not line:
                    continue
                a, b = line.split()
                f.write(f'    .{{ "{escape_for_zig(a)}", "{escape_for_zig(b)}" }},\n')
        f.write("};\n")

    with open(f"{zig_output_dir}/decoder_map.zig", "w", encoding="utf-8") as f:
        f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
        f.write('// Generated by export_tokenizer_zig.py\n\n')
        f.write("pub const decoder_map = [_][]const u8{\n")
        for token, _ in sorted_vocab_by_id:
            f.write(f'    "{escape_for_zig(token)}",\n')
        f.write("};\n")

    with open(f"{zig_output_dir}/encoder_map.zig", "w", encoding="utf-8") as f:
        f.write('// ⚠️ This file is autogenerated. Do not modify directly.\n')
        f.write('// Generated by export_tokenizer_zig.py\n\n')
        f.write('const std = @import("std");\n')
        f.write('const StaticStringMap = std.StaticStringMap;\n')
        f.write('const encoder_data = @import("encoder_data.zig").encoder_data;\n\n')
        f.write('pub const encoder_map = StaticStringMap(usize).initComptime(encoder_data);\n')

if __name__ == "__main__":
    download_tokenizer_files()
    tokenizer = train_tokenizer()
    export_zig_files(tokenizer)
